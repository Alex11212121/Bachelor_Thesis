{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Table of contents\n",
    "\n",
    "1.  [Introduction](#introduction)\n",
    "2.  [Data Filtering](#paragraph1)\n",
    "3.  [Outlier Analysis](#paragraph2)\n",
    "4.  [Complementing Data](#paragraph3)\n",
    "5.  [Data Bias](#paragraph4)\n",
    "\n",
    "### Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "The data provided records all the listings on ImmoScout from 2004 to\n",
    "2015. Since these listings were entered manually by platform users, they\n",
    "contain many incorrect and missing values.\n",
    "\n",
    "This Annexe is designed to be viewed on Jupyter Notebook. All graphs and\n",
    "maps are interactive, providing additional valuable insights into the\n",
    "data. The file has been sent by email in complement but is also\n",
    "accessible at GITHUB LINK. Several adjustments have been made to allow a\n",
    "pdf export of this documentation. These will be mentioned every time.\n",
    "\n",
    "``` python\n",
    "#Importing used libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import statsmodels.graphics.api as smg\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import mahalanobis \n",
    "import folium\n",
    "from folium import plugins\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from sklearn.neighbors import KDTree\n",
    "import plotly.io as pio\n",
    "from IPython.display import Image\n",
    "```\n",
    "\n",
    "### Data Filtering <a name=\"paragraph1\"></a>\n",
    "\n",
    "We first take a look at the data in General. Here the file that is\n",
    "loaded is not the original file but already has certain values filtered\n",
    "out (that will be explained in further steps). This has been done as the\n",
    "original file was too large to be handled by Jupyter Notebook.\n",
    "\n",
    "``` python\n",
    "df = pd.read_csv('Step_01.csv', nrows=100000)\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "print('Information:', df.info())\n",
    "```\n",
    "\n",
    "We see that there are 51 variables with 2’225’232 lines. We now check\n",
    "for missing values.\n",
    "\n",
    "``` python\n",
    "df.isna()\n",
    "```\n",
    "\n",
    "As we can see, multiple “True” means a certain number of missing values.\n",
    "Given a large number of observations, we will remove observations that\n",
    "have missing values. Looking at the raw data in excel, it was also\n",
    "noticed that many prices were incorrect values—formats such as 10,000 or\n",
    "10.000.00 instead of 10’000. The clean_price function removes\n",
    "non-numeric or non-decimal point characters from the string. Afterward,\n",
    "it removes the trailing decimal point if present. If the price is\n",
    "invalid, the function returns an empty string. Otherwise, the function\n",
    "returns the cleaned price. Finally, it checks if prices are correct by\n",
    "limiting the number of digits to 12. The function also checks that a\n",
    "comma does not separate the prices. This is necessary as the original\n",
    "was not a CSV format.\n",
    "\n",
    "``` python\n",
    "def clean_price(price):\n",
    "    price = re.sub(r\"[^0-9.]\", \"\", price or \"\").rstrip(\".\")\n",
    "\n",
    "    # check whether price is valid\n",
    "    if len(price) > 12 or price.count(\".\") > 1:\n",
    "        return \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "\n",
    "def clean_row(row):\n",
    "    row[\"selling_price\"] = clean_price(row[\"selling_price\"])\n",
    "    row[\"a_netm_mon\"] = clean_price(row[\"a_netm_mon\"])\n",
    "    return row\n",
    "```\n",
    "\n",
    "The following snippet fulfills three functions. It applies the functions\n",
    "defined above to remove rows that contain invalid data. However, only\n",
    "the columns of interest are kept. The original dataset contained 52\n",
    "variables; however, only ten were kept for two reasons. First, some of\n",
    "these variables were not of interest. The text description, for example,\n",
    "could not be integrated into a regression analysis (NLP was considered\n",
    "but seemed out of the scope of this thesis). Secondly, certain variables\n",
    "had too many missing variables, which would have considerably reduced\n",
    "the number of observations. Finally, this script only adds rental offers\n",
    "to the new file “Rent_S1_Done.csv”. The variables of interest kept in\n",
    "the final CSV file are listed under the *usecols* parameter.\n",
    "\n",
    "``` python\n",
    "with open(\"adScanFull.csv\") as readfile:  # Name of the original file to filter\n",
    "    with open(\"Step_02.csv\", \"w\") as csvfile:  # Name of the new file name.\n",
    "        reader = csv.DictReader(readfile, delimiter=\"#\")\n",
    "        writer = csv.DictWriter(\n",
    "            csvfile, fieldnames=reader.fieldnames, extrasaction=\"ignore\"\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        for num, row in enumerate(reader):\n",
    "            if num % 10000 == 0:\n",
    "                print(f\"{num} lines processed.\")\n",
    "            row = clean_row(row)\n",
    "            if (\n",
    "                row[\"deal\"]\n",
    "                and row[\"a_netm_mon\"]\n",
    "                and row[\"a_surface_living\"]\n",
    "                and row[\"a_nb_rooms\"]\n",
    "                and row[\"a_sicht\"]\n",
    "                and row[\"a_ofen\"]\n",
    "                and row[\"a_balkon\"]\n",
    "                and row[\"a_baup\"]\n",
    "                and row[\"g_day\"]\n",
    "                and row[\"longitude2\"]\n",
    "                and row[\"latitude2\"]\n",
    "            ):\n",
    "                # Variables which can't have NaN valuies to be included in the\n",
    "                # new dataframe.\n",
    "                writer.writerow(row)\n",
    "                # Above this line are the variables for which, if there is nop\n",
    "                # value, the row of data (offer) is not included in the\n",
    "                # data frame.\n",
    "\n",
    "df_rent = pd.read_csv(\n",
    "    \"Step_02.csv\",\n",
    "    usecols=[\n",
    "        \"deal\",\n",
    "        \"a_netm_mon\",\n",
    "        \"a_surface_living\",\n",
    "        \"a_nb_rooms\",\n",
    "        \"a_sicht\",\n",
    "        \"a_ofen\",\n",
    "        \"a_balkon\",\n",
    "        \"a_baup\",\n",
    "        \"g_day\",\n",
    "        \"longitude2\",\n",
    "        \"latitude2\",\n",
    "    ],\n",
    "    low_memory=False,\n",
    ")\n",
    "# check the types\n",
    "print(df_rent.dtypes)\n",
    "\n",
    "options = [\n",
    "    (\"RENT\")\n",
    "]  # Above this line are the variables which are included in the new dataframe.\n",
    "df_rent = df_rent.loc[df_rent[\"deal\"].isin(options)]\n",
    "df_rent.to_csv(\"Rent_S1_Done.csv\")\n",
    "```\n",
    "\n",
    "Like mentionned previously, the original file was not loaded on\n",
    "JupyterNotebook as it was too large. The code was run on a seperate\n",
    "platrom, the resulting file was uploaded to JupyterNotebook. Bellow the\n",
    "resulting file is explored. We can see the list of variables. The data\n",
    "types are consistent and there are no null values.\n",
    "\n",
    "``` python\n",
    "df = pd.read_csv('Rent_S1_Done.csv', nrows=100000)\n",
    "print('Shape:', df.shape)\n",
    "print('Information:', df.info())\n",
    "```\n",
    "\n",
    "### Outliers analysis <a name=\"paragraph2\"></a>\n",
    "\n",
    "In this following chapter, a series of outlier detection methods will be\n",
    "performed on the data. Since our data was most probably manually entered\n",
    "by individual sellers (on the Immoscout website since it’s a\n",
    "peer-to-peer platform), the chances of having outliers or wrong values\n",
    "are quite high. Additionally, when looking for apartments I have\n",
    "personally seen on Immoscout ads with a rent of zero typed in and the\n",
    "description instructing to contact the oﬀice for the real rent. They do\n",
    "this to allow the ad to be visible to as many people as possible even if\n",
    "the user filters results by price. There are many deviations like this\n",
    "one thus, the reason for the following thorough outlier analysis\n",
    "procedure. In the first step, we load the file from the previous step\n",
    "and parse the dates so that it is in a standardized python format.\n",
    "\n",
    "``` python\n",
    "#date parser setup\n",
    "dateparse = lambda x: datetime.strptime(x, '%Y%m%d') \n",
    "dateparse2 = lambda x: datetime.strptime(x, '%Y')\n",
    "#reading the file\n",
    "df = pd.read_csv('Rent_S1_Done.csv', parse_dates=['g_day'],date_parser=dateparse)\n",
    "```\n",
    "\n",
    "We start by removing columns which are not neede, such as the type of\n",
    "deal, since we only have rental offers in our file. We also remove the\n",
    "index from the old file.\n",
    "\n",
    "``` python\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop('deal', axis=1)\n",
    "```\n",
    "\n",
    "**Net rents**\n",
    "\n",
    "First, the distribution function and a boxplot of the net rents are\n",
    "observed.\n",
    "\n",
    "``` python\n",
    "#Figure 1 \n",
    "fig = px.histogram(df, x=\"a_netm_mon\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Rent\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Rent\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_1.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_1.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "As Figure 1 shows there seem to be some negative values for the rents\n",
    "which must be taken out. After taking out the negative samples the new\n",
    "distribution is displayed in Figure 2.\n",
    "\n",
    "``` python\n",
    "#Figure 2\n",
    "fig = px.histogram(df, x=\"a_netm_mon\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Rent\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Rent\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_2.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_2.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "We still have many results that lie on the extremes of the distribution\n",
    "curve (and outside of the first and last quartile). The upper fence is\n",
    "at 31.52, while the most extreme to the right value is at 450. These\n",
    "values could be very prestigious goods, but the fact that there were\n",
    "only ten transactions at over 171.42 CHF per sqm makes us think they are\n",
    "negligible in the illustration of preferences. On the other hand, one\n",
    "could argue that the data set is biased given its nature (peer-to-peer\n",
    "platform) and that these values shout be oversampled. For this thesis,\n",
    "they will not be considered. However, extreme values are not necessarily\n",
    "outliers. To verify this within the context of this study, it would be\n",
    "wise to evaluate the rent per sqm; as the living surface is the primary\n",
    "driver of price, it makes more sense to look at the rent in relation to\n",
    "at least one other attribute. As figure 3 shows, there are still plenty\n",
    "of values outside of the lower and upper fence.\n",
    "\n",
    "``` python\n",
    "#Adding price per square meter to evaluate ratio realism\n",
    "df['psqm'] = df.apply(lambda row: row.a_netm_mon / row.a_surface_living, axis=1)\n",
    "\n",
    "#Defining the upper and lower maximum value of price per square meter\n",
    "df = df[df.psqm < 170]\n",
    "\n",
    "#Figure 3\n",
    "fig = px.histogram(df, x=\"psqm\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of rent per Square Meter\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Rent per Square Meter\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_3.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_3.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "**Build Year**\n",
    "\n",
    "The following variable is the year the good was built. One challenge was\n",
    "the hidden NaN values under the year “9999” These had to be removed\n",
    "again, reducing the sample size. This is clear in figure 4, where we can\n",
    "see many observations at 9999. There are shy of 700’000 data samples\n",
    "(for a total of 1.3 million), with the building year set to 9999. In\n",
    "simple OLS regressions, the model was more precise with a larger data\n",
    "sample than without the build year as a variable.\n",
    "\n",
    "``` python\n",
    "\n",
    "#Figure 4\n",
    "fig = px.histogram(df, x=\"a_baup\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of year of construction\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Year of construction\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_4.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_4.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "Additionally, the building years are not a continuous variable but a\n",
    "categorical one. This would become a problem in the regression as the\n",
    "large number of categories would be diﬀicult to analyze. The categories\n",
    "are visible in table 1.\n",
    "\n",
    "``` python\n",
    "#Table 1\n",
    "\n",
    "a_baup = {1400:'1400-1799', 1800:'1800-1899', 1900:'1900-1924', 1925: '1925-1949', 1950:'1950-1959', 1960:'1960-1969', 1970:'1970-1979', 1980: '1980-1989', 1990:'1990-1994', 1995:'1995-1999', 2000:'2000-2004', 2005: '2005-2010'}\n",
    "\n",
    "print(\"+------+-------------+\")\n",
    "print(\"| Code |   Period    |\")\n",
    "print(\"+------+-------------+\")\n",
    "for code, period in a_baup.items(): print(f\"| {code:<4} | {period:<11} |\")\n",
    "print(\"+------+-------------+\")\n",
    "```\n",
    "\n",
    "Thus the variable was transformed to a continuous variable. Given the\n",
    "large sameple size, randomly assigning specific dates to observations\n",
    "(within their specified periode) had no effect on an OLS regression\n",
    "model that was used to test if this modification had an effect. This\n",
    "function will only be applied to the data at the end of the outlier\n",
    "analysis to not alter the outputs of further multifactor analysis.\n",
    "\n",
    "``` python\n",
    "# Add new column for building year, choosing number at random\n",
    "\n",
    "build_periods = {\n",
    "    1400: (1400, 1799),\n",
    "    1800: (1800, 1899),\n",
    "    1900: (1900, 1924),\n",
    "    1925: (1925, 1949),\n",
    "    1950: (1950, 1959),\n",
    "    1960: (1960, 1969),\n",
    "    1970: (1970, 1979),\n",
    "    1980: (1980, 1989),\n",
    "    1990: (1990, 1994),\n",
    "    1995: (1995, 1999),\n",
    "    2000: (2000, 2004),\n",
    "    2005: (2005, 2010),\n",
    "}\n",
    "\n",
    "\n",
    "def random_baup(row):\n",
    "    a_baup = row[\"a_baup\"]\n",
    "\n",
    "    if a_baup not in build_periods:\n",
    "        return a_baup\n",
    "\n",
    "    begining, end = build_periods[a_baup]\n",
    "    return random.randrange(begining, end + 1)\n",
    "```\n",
    "\n",
    "**Living Surface**\n",
    "\n",
    "The same procedure as for the rent is carried out for the living\n",
    "surface. Intuitively many of the outliers have already been taken out in\n",
    "the price-to-square-meter analysis.\n",
    "\n",
    "``` python\n",
    "#Figure 5\n",
    "fig = px.histogram(df, x=\"a_surface_living\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Living Surface\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Living Surface\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_5.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_5.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "Similarly to the rent, there are still a relatively large number of data\n",
    "points far beyond the upper fence. This however as well does not\n",
    "necessarily mean they are outliers. But looking at these points again\n",
    "from a rent-to-sqm perspective it is clear that there are many data\n",
    "points with prices per sqm of less than 2 CHF. Additionally, on google\n",
    "maps satelite view, many of these extreme data points find themselves in\n",
    "city centers where this price point is very unlikely. Otherwise, some of\n",
    "them were industrial buildings, possibly indicating a warehouse or oﬀice\n",
    "space but unlikely residential housing. After a meticulous case-by-case\n",
    "inspection, all data points with a rent to sqm ratio of less than 8 (14\n",
    "being the minimum on the 1 October 2022 on the home gate) were\n",
    "eliminated. The distribution in figure 6 seems much more realistic than\n",
    "in figure 5.\n",
    "\n",
    "``` python\n",
    "#Figure 6\n",
    "fig = px.histogram(df, x=\"a_surface_living\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Living Surface\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Living Surface\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_6.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_6.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "**Number of rooms**\n",
    "\n",
    "Figure 7 shows a good to have 10 million rooms, which seems unlikely so\n",
    "it is removed.\n",
    "\n",
    "``` python\n",
    "#Figure 7\n",
    "fig = px.histogram(df, x=\"a_nb_rooms\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Number of Rooms\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Number of Rooms\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_7.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_7.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "As we can see, the distribution is now more realistic; there are still\n",
    "about 2000 samples with more than 8.5 rooms. Performing a case-by-case\n",
    "analysis on google maps, most of them are mistakes. However, some seem\n",
    "real, as the satellite view makes big houses visible. As mentioned\n",
    "before, there is a low amount of high-end properties in the data set;\n",
    "thus, eliminating all goods with over 8.5 rooms would make that even\n",
    "worse. Two thousand entries over 1.3 million are not overly significant.\n",
    "Thus they are not removed. Several lines with non-standard room numbers\n",
    "(4.4, 5.7, etc.) were also removed.\n",
    "\n",
    "``` python\n",
    "df = df[df.a_nb_rooms < 30] \n",
    "df.drop(df.loc[df['a_nb_rooms']==1.04].index, inplace=True)\n",
    "df.drop(df.loc[df['a_nb_rooms']==1.07].index, inplace=True) \n",
    "df.drop(df.loc[df['a_nb_rooms']== 0.5].index, inplace=True)\n",
    "```\n",
    "\n",
    "``` python\n",
    "#Figure 8\n",
    "fig = px.histogram(df, x=\"a_surface_living\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Number of Rooms\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Number of Rooms\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_8.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_8.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "**View, Balcony and Region**\n",
    "\n",
    "The view takes -1 as a value when it is unknown whether there is a view.\n",
    "Eliminating all data which has -1 would be quite a significant loss.\n",
    "Thus it is replaced with 0 which stands for no view. It also seems\n",
    "unlikely that an advertiser would forget to say that his property has a\n",
    "nice view. Otherwise, outliers in these categorical variables are\n",
    "diﬀicult to detect with single-factor methods.\n",
    "\n",
    "``` python\n",
    "df[\"a_sicht\"] = df[\"a_sicht\"].replace(-1, 0)\n",
    "```\n",
    "\n",
    "**Mahala Nobis distance**\n",
    "\n",
    "At this point I built some regressions models and did not get an R value\n",
    "above 0.5565. The model was tunned extensively but did not improve. Thus\n",
    "a second outlier elimination process was started. Mahala Nobis distance\n",
    "method as it is a multivariate method. This method in this context makes\n",
    "sense as some outliers may not be apparent when looking at a single\n",
    "variable or even two but when looking a 4 at the same time they may\n",
    "become more apparent. For example, it would be diﬀicult and time\n",
    "consuming to identify outliers with regards to rents in specific\n",
    "locations and taking into account their living surface. The four most\n",
    "significant variables were taken. Rents, Living surface, age of the\n",
    "building and location. The basic concept of this method is to analyse\n",
    "the distance of the observation from the central tendency and the\n",
    "covariance between the variables.\n",
    "\n",
    "``` python\n",
    "\n",
    "# Select the variables for testing multivariate outliers\n",
    "variables = ['a_netm_mon', 'a_surface_living', 'a_baup', 'longitude2', 'latitude2']\n",
    "data = df[variables]\n",
    "\n",
    "# Calculate the mean and covariance matrix of the data\n",
    "mean = data.mean()\n",
    "covariance = data.cov()\n",
    "\n",
    "# Calculate the Mahalanobis distance for each data point\n",
    "distances = []\n",
    "for index, row in data.iterrows():\n",
    "    distance = mahalanobis(row, mean, covariance)\n",
    "    distances.append(distance)\n",
    "\n",
    "# Set a threshold for outliers based on the chi-squared distribution\n",
    "threshold = np.percentile(distances, 95)\n",
    "\n",
    "# Identify outliers as those with a distance greater than the threshold\n",
    "outliers = []\n",
    "for i, distance in enumerate(distances):\n",
    "    if distance > threshold:\n",
    "        outliers.append(i)\n",
    "\n",
    "# Remove outliers from the dataframe\n",
    "df_cleaned = df.drop(df.index[outliers])\n",
    "\n",
    "# Print the number of outliers removed\n",
    "#print('Number of outliers removed:', len(outliers))\n",
    "\n",
    "df = df_cleaned\n",
    "```\n",
    "\n",
    "The following distribution resulted of this procedure. 71366\n",
    "observations were removed at a 95% threshold based on the chi-squared\n",
    "distribution.\n",
    "\n",
    "``` python\n",
    "#Figure 9\n",
    "fig = px.histogram(df, x=\"a_surface_living\", marginal=\"box\")\n",
    "\n",
    "# Dimensions\n",
    "width_inches = 8.01\n",
    "height_inches = 5\n",
    "\n",
    "#Annotations\n",
    "fig.update_layout(\n",
    "    legend=dict(font=dict(size=16)),\n",
    "    title=dict(text=\"Distribution of Rent\", font=dict(size=20)),\n",
    "    xaxis=dict(title=dict(text=\"Rent\", font=dict(size=16))),\n",
    "    yaxis=dict(title=dict(text=\"Count\", font=dict(size=16))),\n",
    ")\n",
    "\n",
    "# DPI for printing\n",
    "dpi = 300\n",
    "\n",
    "# Save the figure as a PNG image\n",
    "pio.write_image(fig, \"fig_9.png\", width=int(width_inches * dpi), height=int(height_inches * dpi))\n",
    "\n",
    "# Display the saved image in the notebook\n",
    "Image(filename=\"fig_9.png\")\n",
    "\n",
    "#To view interactive plot in JupyterNotebook, uncomment the next line.\n",
    "#fig.show()\n",
    "```\n",
    "\n",
    "The same simple regression improved the adjusted R square from 0.5551 to\n",
    "0.5757. Moreover, before this procedure, transforming the living surface\n",
    "as a log did not improve the mode, which intuitively is strange\n",
    "(source). After this procedure, transforming the living surface into a\n",
    "log improved the model. The same result is observed for the age of the\n",
    "building when squaring it.\n",
    "\n",
    "### Complementing Data <a name=\"paragraph3\"></a>\n",
    "\n",
    "The raw data included the coordinates of each listing. In order to\n",
    "classify each observation to one of the three Swiss regions, a K-Nearest\n",
    "Neighbor algorithm was performed on the data. This algorithm sets\n",
    "different points (reference points), each belonging to a class. The\n",
    "reference points were defined manually, visible in figure 10.\n",
    "\n",
    "``` python\n",
    "#Figure 10\n",
    "\n",
    "#Interactive map code --\n",
    "column_names = [\"STATION NAME\", \"longitude2\", \"latitude2\"]\n",
    "\n",
    "\n",
    "German = [\n",
    "(\"Zurich\",8.5391825,47.3686498),#0\n",
    "(\"St. Gallen\",9.3787173,47.4244818),#1\n",
    "(\"Bern\",7.4474,46.9480),#2\n",
    "(\"Munsingen\",7.5628,46.8747),#3\n",
    "(\"Thun\",7.6280,467580),#4\n",
    "(\"Frutigen\",7.6469,46.5898),#5\n",
    "(\"Wattenwill\",7.5098,46.7699),#6\n",
    "(\"Wimmis\",7.6386,46.6761),#7\n",
    "(\"Interlaken\",7.8632,46.6863),#8\n",
    "(\"Leuk\",7.6346,46.3169),#9\n",
    "(\"Leukerbad\",7.6288,46.3800),#10\n",
    "(\"St-niklaus\",7.8046,46.1762),#11\n",
    "(\"Zermatt\",7.4455,46.0111),#12\n",
    "(\"Lucerne\",8.3093,47.0502),#13\n",
    "(\"Bale\",7.5886,47.5596),#14\n",
    "(\"Coire\",9.5320,46.8508),#15\n",
    "(\"Aldorf\",8.6428,46.8821),#16\n",
    "(\"wassen\",8.5999,46.7070),#17\n",
    "(\"Ilanz\",9.2047,46.7742),#18\n",
    "(\"Splugen\",9.3210,46.5491),#19\n",
    "(\"Brig\",7.9878,46.3159)#20\n",
    "    \n",
    "]\n",
    "French = [\n",
    "(\"Geneva\",6.153438,46.201664),#21\n",
    "(\"Montreux\",6.9106799,46.4312213),#22\n",
    "(\"Lausanne\",6.6322734,46.5196535),#23\n",
    "(\"Aigle\",6.9667,46.3167),#24\n",
    "(\"Bulle\",7.0577268,46.6154512),#24\n",
    "(\"Yverdons\",6.641183,46.7784736),#25\n",
    "(\"Neuchatel\",6.931933,46.992979),#26\n",
    "(\"La Chaux-de-Fonds\",6.8328,47.1035),#27\n",
    "(\"Orsières\",7.1471,46.0282),#28\n",
    "(\"Saignelégier\",6.9964,47.2562),#29\n",
    "(\"Bassecourt\",7.2427,47.3389),#30\n",
    "(\"Paverne\",6.9406,46.8220)#31\n",
    "]\n",
    "\n",
    "Italian = [\n",
    "(\"Lugano\",8.952130,46.004644),#32\n",
    "(\"Locarno\",8.795714,46.168683),#33\n",
    "(\"Fusio\",8.6500,46.4333),#34\n",
    "(\"Faido\",8.8010,46.4782),#35\n",
    "(\"Acquarossa\",8.9398,46.4546),#36\n",
    "(\"Biasca\",8.9705,46.3580),#37\n",
    "(\"Cevio\",8.6023,46.3177),#38\n",
    "(\"Bellinzona\",9.0244,46.1946)#39\n",
    "]\n",
    "\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "m = folium.Map([46.8, 8.33], zoom_start=5)\n",
    "\n",
    "\n",
    "\n",
    "French = pd.DataFrame(French, columns = column_names)\n",
    "German = pd.DataFrame(German, columns = column_names)\n",
    "Italian = pd.DataFrame(Italian, columns = column_names)\n",
    "\n",
    "French_transac = pd.read_csv(\"fre_rent.csv\") \n",
    "Italian_transac = pd.read_csv(\"ita_rent.csv\") \n",
    "German_transac = pd.read_csv(\"ger_rent.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#The K cities\n",
    "for index, row in French.iterrows():\n",
    "   folium.CircleMarker([row['latitude2'],row['longitude2']],\n",
    "                    radius=7,\n",
    "                    #popup=row['Density'],\n",
    "                    fill_color=\"blue\", # divvy color\n",
    "                    fill_opacity=0.5,\n",
    "                    color = 'blue'\n",
    "                   ).add_to(m)\n",
    "\n",
    "for index, row in German.iterrows():\n",
    "   folium.CircleMarker([row['latitude2'],row['longitude2']],\n",
    "                    radius=7,\n",
    "                    #popup=row['Density'],\n",
    "                    fill_color=\"green\", # divvy color\n",
    "                    fill_opacity=0.5,\n",
    "                    color = 'green',\n",
    "                   ).add_to(m)\n",
    "\n",
    "for index, row in Italian.iterrows():\n",
    "   folium.CircleMarker([row['latitude2'],row['longitude2']],\n",
    "                    radius=7,\n",
    "                    #popup=row['Density'],\n",
    "                    fill_color=\"red\", # divvy color\n",
    "                    fill_opacity=0.5,\n",
    "                    color = 'red',\n",
    "                   ).add_to(m)\n",
    "#Uncomment to see \n",
    "#m\n",
    "\n",
    "#Image map code --\n",
    "\n",
    "image = mpimg.imread(\"map_1.png\")\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "# Display image\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This algorithm is largely based on [the works of Corey\n",
    "Hanson](https://towardsdatascience.com/using-scikit-learns-binary-trees-to-efficiently-find-latitude-and-longitude-neighbors-909979bd)\n",
    "using skit-learn library.\n",
    "\n",
    "``` python\n",
    "points = pd.read_csv(\"Rent_S2_Done.csv\")\n",
    "points_orig = pd.read_csv(\"Rent_S2_Done.csv\")\n",
    "\n",
    "# ------- start of the KNN\n",
    "column_names = [\"STATION NAME\", \"longitude2\", \"latitude2\"]\n",
    "\n",
    "\n",
    "cities = [\n",
    "    (\"Zurich\", 8.5391825, 47.3686498),  # 0\n",
    "    (\"St. Gallen\", 9.3787173, 47.4244818),  # 1\n",
    "    (\"Bern\", 7.4474, 46.9480),  # 2\n",
    "    (\"Munsingen\", 7.5628, 46.8747),  # 3\n",
    "    (\"Thun\", 7.6280, 467580),  # 4\n",
    "    (\"Frutigen\", 7.6469, 46.5898),  # 5\n",
    "    (\"Wattenwill\", 7.5098, 46.7699),  # 6\n",
    "    (\"Wimmis\", 7.6386, 46.6761),  # 7\n",
    "    (\"Interlaken\", 7.8632, 46.6863),  # 8\n",
    "    (\"Leuk\", 7.6346, 46.3169),  # 9\n",
    "    (\"Leukerbad\", 7.6288, 46.3800),  # 10\n",
    "    (\"St-niklaus\", 7.8046, 46.1762),  # 11\n",
    "    (\"Zermatt\", 7.4455, 46.0111),  # 12\n",
    "    (\"Lucerne\", 8.3093, 47.0502),  # 13\n",
    "    (\"Bale\", 7.5886, 47.5596),  # 14\n",
    "    (\"Coire\", 9.5320, 46.8508),  # 15\n",
    "    (\"Aldorf\", 8.6428, 46.8821),  # 16\n",
    "    (\"wassen\", 8.5999, 46.7070),  # 17\n",
    "    (\"Ilanz\", 9.2047, 46.7742),  # 18\n",
    "    (\"Splugen\", 9.3210, 46.5491),  # 19\n",
    "    (\"Brig\", 7.9878, 46.3159),  # 20\n",
    "    (\"Geneva\", 6.153438, 46.201664),  # 21\n",
    "    (\"Montreux\", 6.9106799, 46.4312213),  # 22\n",
    "    (\"Lausanne\", 6.6322734, 46.5196535),  # 23\n",
    "    (\"Aigle\", 6.9667, 46.3167),  # 24\n",
    "    (\"Bulle\", 7.0577268, 46.6154512),  # 24\n",
    "    (\"Yverdons\", 6.641183, 46.7784736),  # 25\n",
    "    (\"Neuchatel\", 6.931933, 46.992979),  # 26\n",
    "    (\"La Chaux-de-Fonds\", 6.8328, 47.1035),  # 27\n",
    "    (\"Orsières\", 7.1471, 46.0282),  # 28\n",
    "    (\"Saignelégier\", 6.9964, 47.2562),  # 29\n",
    "    (\"Bassecourt\", 7.2427, 47.3389),  # 30\n",
    "    (\"Paverne\", 6.9406, 46.8220),  # 31\n",
    "    (\"Lugano\", 8.952130, 46.004644),  # 32\n",
    "    (\"Locarno\", 8.795714, 46.168683),  # 33\n",
    "    (\"Fusio\", 8.6500, 46.4333),  # 34\n",
    "    (\"Faido\", 8.8010, 46.4782),  # 35\n",
    "    (\"Acquarossa\", 8.9398, 46.4546),  # 36\n",
    "    (\"Biasca\", 8.9705, 46.3580),  # 37\n",
    "    (\"Cevio\", 8.6023, 46.3177),  # 38\n",
    "    (\"Bellinzona\", 9.0244, 46.1946),  # 39\n",
    "]\n",
    "cities = pd.DataFrame(cities, columns=column_names)\n",
    "# points = pd.DataFrame(points, columns = column_names)\n",
    "\n",
    "kd = KDTree(cities[[\"longitude2\", \"latitude2\"]].values, metric=\"euclidean\")\n",
    "k = 1\n",
    "distances, indices = kd.query(points[[\"longitude2\", \"latitude2\"]], k=k)\n",
    "\n",
    "s = pd.Series([distances, indices])\n",
    "# s.to_csv(\"s.csv\")\n",
    "\n",
    "points_categorised = pd.DataFrame(points_orig)\n",
    "points_categorised_2 = points_categorised.assign(region=indices)\n",
    "points_categorised_2.to_csv(\"trash.csv\")\n",
    "# Replacing the numbers with the name of the region\n",
    "\n",
    "# Seperating the different regions in three different datasets, commented out option to extract csv file, used for further data exploration.\n",
    "ger = pd.read_csv(\"trash.csv\")\n",
    "ger.drop(ger[ger[\"region\"] > 20].index, inplace=True)\n",
    "# ger.to_csv(\"ger_rent.csv\")\n",
    "\n",
    "\n",
    "fre = pd.read_csv(\"trash.csv\")\n",
    "fre.drop(fre[fre[\"region\"] <= 20].index, inplace=True)\n",
    "fre.drop(fre[fre[\"region\"] > 31].index, inplace=True)\n",
    "# fre.to_csv(\"fre_rent.csv\")\n",
    "\n",
    "ita = pd.read_csv(\"trash.csv\")\n",
    "ita.drop(ita[ita[\"region\"] < 32].index, inplace=True)\n",
    "# ita.to_csv(\"ita_rent.csv\")\n",
    "\n",
    "\n",
    "ger.region = 0\n",
    "fre.region = 1\n",
    "ita.region = 2\n",
    "frames = [ger, fre, ita]\n",
    "\n",
    "\n",
    "result = pd.concat(frames)\n",
    "result = result.drop(\"Unnamed: 0\", axis=1)\n",
    "result = result.drop(\"Unnamed: 0.1\", axis=1)\n",
    "```\n",
    "\n",
    "### Data Bias <a name=\"paragraph4\"></a>\n",
    "\n",
    "The data is suspected to have two biases, first an overall larger number\n",
    "of observations for the German part of Switzerland.\n",
    "\n",
    "``` python\n",
    "\n",
    "df = pd.read_csv('rents_S3_Done.csv')\n",
    "\n",
    "counts = df['region'].value_counts()\n",
    "\n",
    "# Print the counts for each region category\n",
    "print(counts)\n",
    "```\n",
    "\n",
    "The output is: German 114’529 French 6’296 Italian 2’529 As seen, we\n",
    "have much more German data than French or Italian.\n",
    "\n",
    "Moreover we can speculate that this is due to the popularity of the\n",
    "platform in Zurich as 49.5% of the oberservations are withing that\n",
    "refference point in the KNN.\n",
    "\n",
    "``` python\n",
    "\n",
    "\n",
    "#Examining at spatial distribution\n",
    "num_observations_bale = (points_categorised_2['region'] == 14).sum()\n",
    "num_observations_zurich = (points_categorised_2['region'] == 0).sum()\n",
    "total_observations = points_categorised_2.shape[0]\n",
    "\n",
    "percentage_bale = (num_observations_bale / total_observations) * 100\n",
    "percentage_zurich = (num_observations_zurich / total_observations) * 100\n",
    "\n",
    "print(\"Number of observations categorized closest to Bale:\", num_observations_bale)\n",
    "print(\"Percentage of observations categorized closest to Bale: {:.2f}%\".format(percentage_bale))\n",
    "print(\"Number of observations categorized closest to Zurich:\", num_observations_zurich)\n",
    "print(\"Percentage of observations categorized closest to Zurich: {:.2f}%\".format(percentage_zurich))\n",
    "print(\"Total number of observations:\", total_observations)\n",
    "```\n",
    "\n",
    "The second bias in the data is the distribution of rents. We see that\n",
    "given the nature of the ImmoScout24 platform, there aren’t many “high\n",
    "end” goods.\n",
    "\n",
    "``` python\n",
    "\n",
    "df = pd.read_csv(\"rents_S3_Done.csv\")\n",
    "\n",
    "# Precautionary data filtering\n",
    "df = df[(df['latitude2'].notnull()) & (df['longitude2'].notnull())]\n",
    "\n",
    "# Creating dataframe to count number of entries for each location.\n",
    "grouped = df.groupby(['latitude2', 'longitude2']).size().reset_index(name='count')\n",
    "\n",
    "# Heatmap using the coordinates and the count of entries as the intensity value\n",
    "switzerland_map = folium.Map(location=[46.8, 8.3], zoom_start=8)\n",
    "switzerland_map.add_child(folium.plugins.HeatMap(grouped[['latitude2', 'longitude2', 'count']].values, radius=15))\n",
    "switzerland_map.save('heatmap.html')\n",
    "```\n",
    "\n",
    "The output of this code is visisble at . We can see that areas such as\n",
    "the Zurich gold coast or the quaie de Cologny in Geneva have close to\n",
    "zero entries."
   ],
   "id": "55ec4a14-2328-4ac9-96c9-5d4500cc455b"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
